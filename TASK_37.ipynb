{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94d4913-9c3f-4634-9563-2b28ead7bf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haseeb\\anaconda3\\envs\\tfenv\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_spacy(text):\n",
    "    # Process the text\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    # Keep only tokens that are not stop words, not punctuation, and are alphabetic\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222ae94c-8423-4c51-9312-0602e48c23df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: My Name Is ABDUL HASEEB!\n",
      "Cleaned: abdul haseeb\n",
      "\n",
      "Original: Text cleaning is a crucial step before modeling.\n",
      "Cleaned: text cleaning crucial step modeling\n",
      "\n",
      "Original: Stopwords and punctuation should be removed.\n",
      "Cleaned: stopwords punctuation remove\n",
      "\n",
      "Original: Stemming reduces words to their root form.\n",
      "Cleaned: stemming reduce word root form\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"My Name Is ABDUL HASEEB!\",\n",
    "    \"Text cleaning is a crucial step before modeling.\",\n",
    "    \"Stopwords and punctuation should be removed.\",\n",
    "    \"Stemming reduces words to their root form.\"\n",
    "]\n",
    "\n",
    "for t in texts:\n",
    "    print(\"Original:\", t)\n",
    "    print(\"Cleaned:\", preprocess_spacy(t))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
