import numpy as np

# ReLU activation and its derivative
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

# Mean Squared Error
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# Derivative of MSE w.r.t. predictions
def mse_derivative(y_true, y_pred):
    return 2 * (y_pred - y_true) / y_true.size

# Simple Neural Network class
class SimpleNeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        self.lr = learning_rate
        
        # Weights and biases
        self.W1=np.random.randn(input_size, hidden_size)
        self.b1=np.zeros((1, hidden_size))
        self.W2=np.random.randn(hidden_size, output_size)
        self.b2=np.zeros((1, output_size))

    def forward(self, X):
        self.z1 =X @ self.W1 + self.b1
        self.a1 =relu(self.z1)
        self.z2 =self.a1 @ self.W2 + self.b2
        return self.z2  # No activation in output (linear)

    def backward(self, X, y, y_pred):
        loss_grad=mse_derivative(y, y_pred)  # dL/dy_pred

        # Output layer gradients
        dW2=self.a1.T @ loss_grad
        db2=np.sum(loss_grad, axis=0, keepdims=True)

        # Hidden layer gradients
        da1 = loss_grad @ self.W2.T
        dz1 = da1 * relu_derivative(self.z1)
        dW1 = X.T @ dz1
        db1 = np.sum(dz1, axis=0, keepdims=True)

        # Updating weights and biases
        self.W2-=self.lr*dW2
        self.b2-=self.lr*db2
        self.W1-=self.lr*dW1
        self.b1-=self.lr*db1

    def train(self, X, y, epochs=1000):
        for epoch in range(epochs):
            y_pred=self.forward(X)
            loss= mse(y, y_pred)
            self.backward(X, y, y_pred)
            if epoch % 100==0:
                print(f"Epoch {epoch},Loss:{loss:.4f}")

    def predict(self, X):
        return self.forward(X)
